{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49dc7c0a-00c3-488e-b49e-a358fd4e2bb0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv('/FileStore/tables/heart_attack_prediction_dataset.csv', header=True, inferSchema=True)\n",
    "#display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13cc8d07-198a-4bdb-b5f8-8f72340718f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split\n",
    "df = df.withColumn(\"Pressao_Sistolica\", split(df[\"Blood Pressure\"], \"/\")[0].cast(\"int\"))\n",
    "df = df.withColumn(\"Pressao_Diastolica\", split(df[\"Blood Pressure\"], \"/\")[1].cast(\"int\"))\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b151b2c9-a9da-43c6-8f22-3b9e586befbf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "feature_columns = [\n",
    "    'Age', 'Cholesterol', \n",
    "    'Heart Rate', 'Diabetes', 'Family History', 'Smoking', 'Obesity', \n",
    "    'Alcohol Consumption', 'Exercise Hours Per Week', \n",
    "    'Previous Heart Problems', 'Medication Use', 'Stress Level', \n",
    "    'Sedentary Hours Per Day', 'Income', 'BMI', 'Triglycerides', \n",
    "    'Physical Activity Days Per Week', 'Sleep Hours Per Day', 'Pressao_Sistolica','Pressao_Diastolica'\n",
    "    \n",
    "]\n",
    "\n",
    "# Colunas categóricas a serem convertidas\n",
    "string_columns = ['Sex','Diet','Country','Continent','Hemisphere']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#feature_columns = ['Physical Activity Days Per Week','Medication Use','Previous Heart Problems',\n",
    " #                   'Stress Level','Smoking','Age']\n",
    "\n",
    "# Colunas categóricas a serem convertidas\n",
    "#string_columns = ['Sex','Diet', 'Blood Pressure']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Criando um StringIndexer para cada coluna categórica\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column + \"_index\", handleInvalid=\"keep\") for column in string_columns]\n",
    "\n",
    "vec_assembler = VectorAssembler(inputCols=feature_columns + [column + \"_index\" for column in string_columns], outputCol='features')\n",
    "\n",
    "\n",
    "# Criando um pipeline para encadear as etapas\n",
    "pipeline = Pipeline(stages=indexers + [vec_assembler])\n",
    "\n",
    "df_transform = pipeline.fit(df).transform(df)\n",
    "#df_transform.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb3e6a88-0313-4245-9f58-d337d741cb53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "feature_columns = [\n",
    "    'Age', 'Cholesterol', \n",
    "    'Heart Rate', 'Diabetes', 'Family History', 'Smoking', 'Obesity', \n",
    "    'Alcohol Consumption', 'Exercise Hours Per Week', \n",
    "    'Previous Heart Problems', 'Medication Use', 'Stress Level', \n",
    "    'Sedentary Hours Per Day', 'Income', 'BMI', 'Triglycerides', \n",
    "    'Physical Activity Days Per Week', 'Sleep Hours Per Day', 'Pressao_Sistolica','Pressao_Diastolica'\n",
    "]\n",
    "\n",
    "# Colunas categóricas a serem convertidas\n",
    "string_columns = ['Sex','Diet','Country','Continent','Hemisphere']\n",
    "\n",
    "# Criando um StringIndexer para cada coluna categórica\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column + \"_index\", handleInvalid=\"keep\") for column in string_columns]\n",
    "\n",
    "# Criando um OneHotEncoder para cada coluna categórica\n",
    "onehot_encoders = [OneHotEncoder(inputCol=column + \"_index\", outputCol=column + \"_encoded\") for column in string_columns]\n",
    "\n",
    "# Criando um VectorAssembler\n",
    "vec_assembler = VectorAssembler(inputCols=feature_columns + [column + \"_encoded\" for column in string_columns], outputCol='features')\n",
    "\n",
    "# Criando um pipeline para encadear as etapas\n",
    "pipeline = Pipeline(stages=indexers + onehot_encoders + [vec_assembler])\n",
    "\n",
    "# Aplicando o pipeline aos dados\n",
    "df_transform = pipeline.fit(df).transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49c1e6c5-bdd6-4134-81b9-b35486628f6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dbf7c92-a53f-4478-bd5e-fa7828e0b798",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_train, df_test = df_transform.randomSplit([.8, .2], seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91fe137c-f732-4be1-bd32-7258d1cee975",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(maxDepth=4, labelCol='Heart Attack Risk', featuresCol='features', maxBins=30)\n",
    "\n",
    "#ajustando aos dados de treino\n",
    "rf_model = rf.fit(df_train)\n",
    "\n",
    "#vendo tamanho da árvorer\n",
    "tree_structure = rf_model.toDebugString\n",
    "\n",
    "# Divida a string em linhas e conte as linhas\n",
    "num_nodes = len(tree_structure.split('\\n'))\n",
    "\n",
    "# Exiba o número de nós\n",
    "print(f\"Número total de nós na árvore: {num_nodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77337fed-ab3c-4d1b-ad80-5e7a76ec1efd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#realizando previsão\n",
    "df_pred=rf_model.transform(df_test)\n",
    "df_pred = df_pred.withColumnRenamed('prediction','prediction_RF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89e28ed0-dd8b-4acd-ba4e-b6b0ae917f25",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Adicionar coluna com a diferença entre a previsão e o rótulo real\n",
    "df_comparison = df_pred.withColumn(\"prediction_diff\", col(\"prediction_RF\") - col(\"Heart Attack Risk\"))\n",
    "\n",
    "\n",
    "# Contar o número de acertos e erros\n",
    "correct_predictions = df_comparison.filter(col(\"prediction_diff\") == 0).count()\n",
    "incorrect_predictions = df_comparison.filter(col(\"prediction_diff\") != 0).count()\n",
    "\n",
    "# Calcular a acurácia\n",
    "total_predictions = df_comparison.count()\n",
    "accuracy = correct_predictions / total_predictions\n",
    "\n",
    "# Exibir os resultados\n",
    "print(f\"Total de Previsões: {total_predictions}\")\n",
    "print(f\"Número de Acertos: {correct_predictions}\")\n",
    "print(f\"Número de Erros: {incorrect_predictions}\")\n",
    "print(f\"Acurácia: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a11e402-a9df-48cc-a91f-395e55934023",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pred.filter(df_pred['prediction_RF']==0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e7f7903-8e61-42eb-8f44-2d30ef0be5bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# count values in name column\n",
    "#print(df['Heart Attack Risk'].value_counts()[1])\n",
    "df.filter(df['Heart Attack Risk']==0).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ed9c585-9359-4d4b-a37d-4923a0e00fdb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.filter(df['Heart Attack Risk']==1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86825007-c196-4396-8a83-6d5b3405824d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_zero = df.filter(col('Heart Attack Risk') == 0)\n",
    "df_um = df.filter(col('Heart Attack Risk') == 1)\n",
    "#df_um.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec3c5f2e-f9d8-48f0-b652-a6fd983c6d70",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "amostra_zeros = df_zero.sample(False, 1.0).distinct().limit(3139)\n",
    "#\n",
    "#amostra_zeros.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d7cbe96-488e-4dd9-8fef-8d1c6f031a25",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_new = amostra_zeros.union(df_um)\n",
    "df_new.display()\n",
    "#juntando_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79292472-7acb-47c3-8633-9062e179bb87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "\n",
    "feature_columns = [\n",
    "    'Age', 'Cholesterol', \n",
    "    'Heart Rate', 'Diabetes', 'Family History', 'Smoking', 'Obesity', \n",
    "    'Alcohol Consumption', 'Exercise Hours Per Week', \n",
    "    'Previous Heart Problems', 'Medication Use', 'Stress Level', \n",
    "    'Sedentary Hours Per Day', 'Income', 'BMI', 'Triglycerides', \n",
    "    'Physical Activity Days Per Week', 'Sleep Hours Per Day', 'Pressao_Sistolica','Pressao_Diastolica'\n",
    "    \n",
    "]\n",
    "\n",
    "# Colunas categóricas a serem convertidas\n",
    "string_columns = ['Sex','Diet','Country','Continent','Hemisphere']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#feature_columns = ['Physical Activity Days Per Week','Medication Use','Previous Heart Problems',\n",
    " #                   'Stress Level','Smoking','Age']\n",
    "\n",
    "# Colunas categóricas a serem convertidas\n",
    "#string_columns = ['Sex','Diet', 'Blood Pressure']\n",
    "\n",
    "\n",
    "\n",
    "# Criando um StringIndexer para cada coluna categórica\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column + \"_index\", handleInvalid=\"keep\") for column in string_columns]\n",
    "\n",
    "vec_assembler = VectorAssembler(inputCols=feature_columns + [column + \"_index\" for column in string_columns], outputCol='features')\n",
    "\n",
    "# Criando um pipeline para encadear as etapas\n",
    "pipeline = Pipeline(stages=indexers + [vec_assembler])\n",
    "df_transform_new = pipeline.fit(df_new).transform(df_new)\n",
    "#df_transform.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e226b1e0-ce91-42e9-afc7-005fb15137b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#separando os dados de teste dnv\n",
    "df_train_new, df_test_new = df_transform_new.randomSplit([.8, .2], seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac158190-73fc-41da-8a5b-96a506e30f0b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rf_new = RandomForestClassifier(maxDepth=2,labelCol='Heart Attack Risk', featuresCol='features', maxBins=32)\n",
    "\n",
    "#ajustando aos dados de treino\n",
    "rf_model_new = rf_new.fit(df_train_new)\n",
    "\n",
    "#vendo tamanho da árvorer\n",
    "tree_structure_new = rf_model_new.toDebugString\n",
    "\n",
    "# Divida a string em linhas e conte as linhas\n",
    "num_nodes_new = len(tree_structure_new.split('\\n'))\n",
    "\n",
    "# Exiba o número de nós\n",
    "print(f\"Número total de nós na árvore: {num_nodes_new}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d9d5f25-1c01-44b1-af57-ae649bf6869d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7022d81b-549f-4031-bf0a-b214f5287eab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#realizando previsão\n",
    "df_pred_new=rf_model_new.transform(df_test_new)\n",
    "df_pred_new = df_pred_new.withColumnRenamed('prediction','prediction_RF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "957394b7-fa23-4824-bd8b-d6931cbb8e54",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_transform.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70f3609f-73d6-4072-8fa6-9c791a602fb7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "037ee0c9-27cc-44f2-8050-478f01c8ed79",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Adicionar coluna com a diferença entre a previsão e o rótulo real\n",
    "df_comparison_new = df_pred_new.withColumn(\"prediction_diff\", col(\"prediction_RF\") - col(\"Heart Attack Risk\"))\n",
    "\n",
    "\n",
    "# Contar o número de acertos e erros\n",
    "correct_predictions_new = df_comparison_new.filter(col(\"prediction_diff\") == 0).count()\n",
    "incorrect_predictions_new = df_comparison_new.filter(col(\"prediction_diff\") != 0).count()\n",
    "\n",
    "# Calcular a acurácia\n",
    "total_predictions_new = df_comparison_new.count()\n",
    "accuracy_new = correct_predictions_new / total_predictions_new\n",
    "\n",
    "# Exibir os resultados\n",
    "print(f\"Total de Previsões: {total_predictions_new}\")\n",
    "print(f\"Número de Acertos: {correct_predictions_new}\")\n",
    "print(f\"Número de Erros: {incorrect_predictions_new}\")\n",
    "print(f\"Acurácia: {accuracy_new:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdbd7089-17e5-48e2-a05b-51c72142f584",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pred_new.filter(df_pred_new['prediction_RF']==1).count()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Decision tree limpo",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
